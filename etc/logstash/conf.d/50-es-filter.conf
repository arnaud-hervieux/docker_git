filter {
  # PROD
  # ES log type : eslog eslogtracker eslogslow
  if "eslog" in [type] {	
    #multiline {
    #  pattern => "(^net.ldmobile.+)|(^com.+)|(^backtype.+)|(^org.+)|(^java.+)|(^\s)|(^.+Exception: .+)|(^\s+at .+)|(^\s+... \d+ more)|(^Causes by:)|(^\s*Caused by:.+)"
    #  what => "previous"
    #}
    # new way to handle log events
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE:log_date}"
      negate => true
      what => "previous"
    }
    #Ancienne config mise en place
    # logstash timestamp = log timestamp for ordering 
    #grok {
    #   patterns_dir => "/etc/logstash/conf.d/patterns"
    #   match => [ "message", "%{LOG_DATE:log_date}"]
    #}

    #date {
    #    match => ["log_date","YYYY-MM-dd HH:mm:ss,SSS"]
    #}

    #si on parse les slowlogs
    if "logslow" in [type] {
      grok {
       match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{WORD:severity}%{SPACE}\]\[%{DATA:log_source}\]%{SPACE}\[%{DATA:node_name}\]%{SPACE}\[%{DATA:index}\]\[%{DATA:shard}\]%{GREEDYDATA:kv_pairs}" ]
      }
      kv {
        source => "kv_pairs"
        field_split => " \],"
        value_split => "\["
      }
    }
    #si c'est un type eslog ou eslogtracker
    else {
      #on essaye de decouper le path et les params indiques si c est un type d erreur Bad Request avec ou sans param
      #sinon on decoupe par defaut le message es pour extraire le level, la classe et le message d'erreur
      grok {
        #Cas generique
        match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{WORD:severity}%{SPACE}\]\[%{DATA:log_source}\]%{SPACE}%{DATA:content}({({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
        #Cas pour les events bad requests avec params
        match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{WORD:severity}%{SPACE}\]\[%{DATA:log_source}\]%{SPACE}(?<content>(.*Bad Request for ))(?<brq_path>(.*?\?))(?<brq_query>([^\s]+))%{SPACE}%{DATA:stacktrace}({({[^}]+},?\s*)*})?\s*$(?<stacktrace2>(?m:.*))?" ]
        #Cas pour les bad resquests sans param
        match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{WORD:severity}%{SPACE}\]\[%{DATA:log_source}\]%{SPACE}(?<content>(.*Bad Request for ))(?<brq_path>([^\s]+))%{SPACE}%{DATA:stacktrace}({({[^}]+},?\s*)*})?\s*$(?<stacktrace2>(?m:.*))?" ]
      }
    }
    #date {
    #  "match" => [ "timestamp", "YYYY-MM-dd HH:mm:ss,SSS" ]
    #  target => "@timestamp"
    #}
    #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event_time = event['timestamp'].to_s;
        event['system_hour'] = system_time[11,2];
        event['event_hour'] = event_time[11,2];"
    }
    if [event_hour] != [system_hour] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "system_hour","event_hour","stacktrace","tags","offset" ]
      rename => {
       "source" => "source_body"
       "log_source" => "source"
      }
      convert => {
       "took_millis" => "integer"
       "total_shards" => "integer"
       "shard" => "integer"
     }
    }
  }
  # STORM log type : stormlog stormlogsupervisor stormlognimbus
  else if [type] == "stormlog" {
    multiline {
      pattern => "%{TIMESTAMP_ISO8601:timestamp}"
      negate => true
      what => "previous"
    }
    grok {
       match => [ "message", "%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{DATA:process}%{SPACE}\[%{WORD:severity}\]%{SPACE}%{GREEDYDATA:content}({({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # POSTGRESQL log type : pglog
  else if [type] == "pglog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE_COMMON:log_date}"
      negate => true
      what => "previous"
    }
    grok {
       patterns_dir => "/etc/logstash/conf.d/patterns"
       match => [ "message", "%{LOG_DATE_COMMON:timestamp}%{SPACE}UTC%{SPACE}(?<host_exe>([^\s]+))%{SPACE}%{DATA:db}%{SPACE}%{WORD:severity}\:%{SPACE}duration\:%{SPACE}%{NUMBER:duration:float}%{SPACE}ms%{GREEDYDATA:request}({({[^}]+},?\s*)*})?\s*$(?<request2>(?m:.*))?" ]
       match => [ "message", "%{LOG_DATE_COMMON:timestamp}%{SPACE}UTC%{SPACE}(?<host_exe>([^\s]+))%{SPACE}%{DATA:db}%{SPACE}%{WORD:severity}\:%{GREEDYDATA:request}({({[^}]+},?\s*)*})?\s*$(?<request2>(?m:.*))?" ]
       match => [ "message", "%{LOG_DATE_COMMON:timestamp}%{SPACE}UTC%{SPACE}%{WORD:severity}\:%{SPACE}duration\:%{SPACE}%{NUMBER:duration:float}%{SPACE}ms%{GREEDYDATA:request}({({[^}]+},?\s*)*})?\s*$(?<request2>(?m:.*))?" ]
       match => [ "message", "%{LOG_DATE_COMMON:timestamp}%{SPACE}UTC%{SPACE}%{WORD:severity}\:%{SPACE}%{GREEDYDATA:info}" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # SPARK log type : sparklog
  else if [type] == "sparklog" {
    multiline {
      pattern => "(^])|(^org.+)|(^backtype.+)|(^com.+)|(^java.+)|(^\s)|(^.+Exception: .+)|(^\s+at .+)|(^\s+... \d+ more)|(^Causes by:)|(^\s*Caused by:.+)"
      what => "previous"
    }
  }
  # netadgebo log type : bolog parser du fichier logstash-production.log
  else if [type] == "bolog" {
    json {
      source => "message"
    }
   # mutate {
   #   convert => { "duration" => "float" }
   # }
    #si la trace contient les caracteristiques d un check haproxy, on ne le prend pas en consideration dans es car trop nombreux et inutiles
    if "HEAD" in [method] and "html" in [format] and "front" in [controller] and "/" in [path] and [status] == 302 {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # netadgebo log type : boprodlog parser du fichier production.log
  else if [type] == "boprodlog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "[A-Z], \[%{LOG_BO_PRODUCTION:log_date} #[0-9]+\].*"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message", "[A-Z], \[%{LOG_BO_PRODUCTION:log_date} #[0-9]+\] %{WORD:severity} -- :%{GREEDYDATA:content}({({[^}]+},?\s*)*})?\s*$(?<detail>(?m:.*))?" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # netadgebo cron (cron whenever) log type : bocronlog
  else if [type] == "bocronlog" {
    multiline {
      pattern => "\[%{TIMESTAMP_ISO8601:timestamp}\]"
      negate => true
      what => "previous"
    }
    grok {
      match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{DATA:command}\]%{SPACE}%{GREEDYDATA:result}%{GREEDYDATA:content}({({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
    }
    date {
      "match" => [ "timestamp", "YYYY-MM-dd HH:mm:ss" ]
      target => "@timestamp"
    }
    #si la trace contient started ou done on drop
    if [result] in ["Started", "Done.", "Waiting 30 seconds..."] {
      drop {}
    }
    #NE FONCTIONNE PAS !!!
    #puis on ajoute un niveau de log 
    #if "fail" in [result] {
    #   add_field => [ "severity", "ERROR" ]
    #}
    #else if "error" in [result] {
    #   add_field => [ "severity", "ERROR" ]
    #}
    #else {
    #   add_field => [ "severity", "INFO" ]
    #}

    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # netadgebo sidekiq log type : bosidekiqlog 
  else if [type] == "bosidekiqlog" {
    grok {
      match => [ "message","%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{NUMBER:pid}%{SPACE}(?<transaction_id>(TID-[a-zA-Z0-9-]*))%{SPACE}%{WORD:worker}%{SPACE}(?<job_id>(JID-[a-zA-Z0-9-]*))%{SPACE}%{WORD:severity}:%{SPACE}%{GREEDYDATA:info}" ]
      match => [ "message","%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{NUMBER:pid}%{SPACE}(?<transaction_id>(TID-[a-zA-Z0-9-]*))%{SPACE}%{WORD:severity}:%{SPACE}%{GREEDYDATA:info}" ]
      match => [ "message", "(?<info>.*)" ]
    }
    
#    if "INFO" not in [severity] {
#      add_field => [ "severity", "ERROR" ]
#    }

    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # syslog
  else if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }

    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  #druid
  else if [type] == "druidlog" {
    multiline {
      #Une nouvelle info commence par un chiffre (date ou time du gc)
      pattern => "^[0-9]"
      negate => true
      what => "previous"
    }
    grok {
      #filtre special pour Yves souhaitant recupÃ©rer la valeur indiquee dans les events thrownAway
      match => [ "message","%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{WORD:severity}%{SPACE}\[MonitorScheduler-0\]%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}\"events/thrownAway\",\"value\":%{NUMBER:value_thrownaway},%{GREEDYDATA:info_sup}({({[^}]+},?\s*)*})?\s*$(?<stacktrace> (?m:.*))?" ]
      #infos les plus courantes
      match => [ "message","%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{WORD:severity}%{SPACE}\[(?<action>([^\s]+))\]%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}({({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
      #infos des gc
      match => [ "message","%{NUMBER:gc_time}\:%{SPACE}%{GREEDYDATA:info}" ]
    }
    #On supprime tous les logs INFO car vraiment trop verbeux sauf si c'est la metric que Yves veux
    if "INFO" in [severity] and [value_thrownaway] !~ /.+/ {
      drop {}
    }
    #On supprimer aussi tout ce qui gc (pas de severity) car trop trop verbeux
    if "" == [severity] or ![severity] {
      drop {}
    }

    #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event['system_time'] = system_time[0,10];
        event_time = event['timestamp'].to_s;
        event['event_time'] = event_time[0,10];"
    }
    if [event_time] not in [system_time] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      #remove_field => [ "kv_pairs", "timestamp" ]
      remove_field => [ "system_time","event_time","message","tags","offset" ]
    }

    #conversion en entier pour le graph kibana des realtime thrownaway
    mutate {
        convert => [ "value_thrownaway", "integer" ]
    }
  }
  # netadgebo tomcat log type : tomcatlog
  else if [type] == "tomcatlog" {
    multiline {
      #Une nouvelle info commence par une date de type tomcat ou un timestamp de base
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "(^%{LOG_DATE_TOMCAT})|(^%{TIMESTAMP_ISO8601:timestamp})"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{LOG_DATE_TOMCAT:timestamp}%{SPACE}%{JAVACLASS:class}%{SPACE}%{WORD:type_log}\n%{WORD:info}:%{SPACE}%{GREEDYDATA:logmessage}(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?"]
    }
    date {
      match => [ "timestamp", "MMM dd, yyyy HH:mm:ss a" ]
    }

    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  else if [type] == "tomcataccesslog" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      #infos les plus courantes
      match => [ "message","%{IP:client}%{SPACE}-%{SPACE}-%{SPACE}\[%{LOG_DATE_TOMCATACCESS:timestamp}\]%{SPACE}\"%{WORD:method}%{SPACE}%{PATH:path}%{SPACE}%{DATA:http_version}\"%{SPACE}%{NUMBER:status}%{SPACE}%{NUMBER:duration}"]
      #La meme mais sans le duration
      match => ["message","%{IP:client}%{SPACE}-%{SPACE}-%{SPACE}\[%{LOG_DATE_TOMCATACCESS:timestamp}\]%{SPACE}\"%{WORD:method}%{SPACE}%{PATH:path}%{SPACE}%{DATA:http_version}\"%{SPACE}%{NUMBER:status}"]
    }
    #si la trace contient les caracteristiques d un check haproxy, on ne le prend pas en consideration dans es car trop nombreux et inutiles
    if "HEAD" in [method] and [client] == "127.0.0.1" {
      drop {}
    }

    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # netadgebo saiku log type : saikulog
  else if [type] == "saikulog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE:log_date}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{LOG_DATE:timestamp}%{SPACE}%{WORD:severity}%{SPACE}\[%{DATA:class}\]%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<trace>(?m:.*))?"]
    }
    date {
      match => ["log_date","YYYY-MM-dd HH:mm:ss,SSS"]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # redis des brokers
  else if [type] == "redislog" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","\[%{POSINT:pid}\]%{SPACE}%{LOG_DATE_REDIS:timestamp} \* %{GREEDYDATA:info}" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # kafka
  else if [type] == "kafkalog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE:timestamp}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","\[%{LOG_DATE:timestamp}\]%{SPACE}%{WORD:severity}%{SPACE}(?<info>.*)\(%{DATA:process}\)(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # suro
  else if [type] == "surolog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE_SURO:timestamp}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{LOG_DATE_SURO:timestamp}%{SPACE}%{WORD:severity}%{SPACE}(?<class>.*?\:)(?<info>.*)(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
    }
    #On supprime tous les logs INFO car vraiment trop verbeux
    if "INFO" in [severity] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # cassandra output.log
  # On ne prend plus en compte output.log depuis la maj de cassandra en 2.x
  #else if [type] == "cassandraoutlog" {
  #  multiline {
  #    pattern => "^(ERROR| INFO| WARN)"
  #    negate => true
  #    what => "previous"
  #  }
  #  grok {
  #    patterns_dir => "/etc/logstash/conf.d/patterns"
  #    match => [ "message","(?<severity>(ERROR| INFO| WARN))%{SPACE}%{LOG_HOUR_CASSANDRA:timestamp}%{SPACE}(?<info>.*)(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
  #  }
  #  #On supprime tous les logs INFO car vraiment trop verbeux
  #  if "INFO" in [severity] {
  #    drop {}
  #  }
  #}
  # cassandra system.log
  else if [type] == "cassandrasyslog" {
    multiline {
      pattern => "^(ERROR| INFO| WARN)"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","(?<severity>(ERROR| INFO| WARN))%{SPACE}\[(?<process>.*?\])%{SPACE}%{LOG_DATE:timestamp}%{SPACE}%{GREEDYDATA:class}%{SPACE}\(line %{NUMBER:line}\)(?<info>.*)(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
    }
    date {
      match => ["timestamp","YYYY-MM-dd HH:mm:ss,SSS"]
    }
    #si la trace est une info sur un repair on drop car trop d entrees
    if "INFO" in [severity] and [class] in ["RepairJob.java ","RepairSession.java ","CompactionTask.java ","Memtable.java ","ColumnFamilyStore.java ","StreamingRepairTask.java ","StreamResultFuture.java ","Differencer.java ","Validator.java ", "StorageService.java ","SSTableReader.java "] {
      drop {}
    }
    #if "WARN" in [severity] and [class] in ["BatchStatement.java "] {
    #  drop {}
    #}
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # zookeeper : zookeeper.out.log
  else if [type] == "zookeeperoutlog" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{GREEDYDATA:action}\:%{SPACE}%{LOG_DATE_TOMCAT:timestamp}%{SPACE}%{GREEDYDATA:message}" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # zookeeper : zookeeper.log
  else if [type] == "zookeeperlog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE:timestamp}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      # format zookeeper standalone
      match => [ "message","%{LOG_DATE_ZOOKEEPER:timestamp}%{SPACE}\-%{SPACE}%{WORD:severity}%{SPACE}\[(?<process>.*?\])%{SPACE}\-%{SPACE}(?<info>.*)(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
      # format zookeeper multi servers
      match => [ "message","%{LOG_DATE_ZOOKEEPER:timestamp}%{SPACE}\[myid\:[0-9]\]%{SPACE}\-%{SPACE}%{WORD:severity}%{SPACE}\[(?<process>.*?\])%{SPACE}\-%{SPACE}(?<info>.*)(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
    }
    date {
      match => ["timestamp","YYYY-MM-dd HH:mm:ss,SSS"]
    }
    #On supprime tous les logs INFO car vraiment trop verbeux
    if "INFO" in [severity] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # druidindexer  pour job spark : druidindexer.log 
  else if [type] == "druidindexer" {
    grok {
      match => [ "message","%{NUMBER:fromDate},%{GREEDYDATA:output},%{GREEDYDATA:input},%{NUMBER:toDate},%{WORD:job},%{NUMBER:timestamp},%{WORD:metric_name},%{NUMBER:metric_value}" ]
    }
    #conversion du timestamp en date ISO
    date {
      match => [ "timestamp", "UNIX_MS" ]
      target => "@timestamp"
    }
    #conversion en entier pour le graph kibana
    mutate {
        convert => [ "metric_value", "integer" ]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # store-refiner pour job partiel
  else if [type] == "storerefiner" {
     json {
      source => "message"
     }

     # On supprime les champs qui ne nous interesse pas
    mutate {
       remove_field => [ "message","tags","offset" ]
    }
  }
  # rtb-grinder
  else if [type] == "grinderlog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
#      pattern => "(%{LOG_DATE_COMMON:log_date})"
      pattern => "(^%{LOG_DATE_TOMCAT})|(^%{LOG_DATE_COMMON:log_date})"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{LOG_DATE_COMMON:timestamp}%{SPACE}%{WORD:severity}%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<trace>(?m:.*))?"]
      match => [ "message","%{LOG_DATE_TOMCAT:timestamp}%{SPACE}%{JAVACLASS:class}%{SPACE}%{WORD:type_log}\n%{WORD:severity}:%{SPACE}%{GREEDYDATA:logmessage}(\n{({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?"]
#      match => [ "message","%{LOG_DATE_TOMCAT:timestamp}%{SPACE}%{WORD:severity}%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<trace>(?m:.*))?"]
    }
    #On supprime tous les logs INFO de zeno car vraiment trop verbeux
    if "INFO" in [severity] and "AmazonZenoDOP" in [class] {
      drop {}
    }
     #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event['system_time'] = system_time[0,10];
        event_time = event['timestamp'].to_s;
        event['event_time'] = event_time[0,10];"
    }
    if [event_time] not in [system_time] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "system_time","event_time","message","tags","offset" ]
    }
  }
  # rtb-percolator
  else if [type] == "percolatorlog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE_COMMON:log_date}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{LOG_DATE_COMMON:timestamp}%{SPACE}%{WORD:severity}%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<trace>(?m:.*))?"]
    }
    date {
      match => ["log_date","YYYY-MM-dd HH:mm:ss"]
    }
    #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event_time = event['timestamp'].to_s;
        event['system_hour'] = system_time[11,2];
        event['event_hour'] = event_time[11,2];"
    }
    if [event_hour] != [system_hour] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "system_hour","event_hour","message","tags","offset" ]
    }
    #if "Something went wrong" in [info] {
    #  drop {}
    #}
  }
  # rtb-svc-seer
  else if [type] == "seerlog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE_COMMON:log_date}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","%{LOG_DATE_COMMON:timestamp}%{SPACE}%{WORD:severity}%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<trace>(?m:.*))?"]
    }
    date {
      match => ["log_date","YYYY-MM-dd HH:mm:ss"]
    }
    #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event_time = event['timestamp'].to_s;
        event['system_day'] = system_time[8,2];
        event['event_day'] = event_time[8,2];
        event['system_hour'] = system_time[11,2];
        event['event_hour'] = event_time[11,2];"
    }
    if [event_hour] != [system_hour] or [event_day] != [system_day] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "system_hour","event_hour","system_day","event_day","message","offset" ]
    }
  }
  # consul
  else if [type] == "consulagentlog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "(    %{LOG_DATE_CONSUL:timestamp})|(==>)"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message","    %{LOG_DATE_CONSUL:timestamp}%{SPACE}\[%{WORD:severity}\]%{SPACE}%{DATA:module}:%{SPACE}%{GREEDYDATA:info}"]
      match => [ "message","==>%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<info_supp>(?m:.*))?"]
    }
    #Trop verbeux, on drop
    if "INFO" in [severity] {
      drop {}
    }
    #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    #comme les dates n ont pas le meme format, on compare que sur le jour
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event_time = event['timestamp'].to_s;
        event['system_hour'] = system_time[11,2];
        event['event_hour'] = event_time[11,2];"
    }
    if [event_hour] != [system_hour] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "system_hour","event_hour","message","offset" ]
    }
  }
  # audience
  else if [type] == "audiencelog" {
    multiline {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      pattern => "%{LOG_DATE_COMMON:timestamp}"
      negate => true
      what => "previous"
    }
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message", "%{LOG_DATE_COMMON:timestamp}%{SPACE}%{WORD:severity}%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}(\n{({[^}]+},?\s*)*})?\s*$(?<trace>(?m:.*))?"]
    }
    #On regarde si l event n est pas trop vieux, en quel cas on le supprime
    ruby {
      init => "require 'time'"
      code => "system_time = event['@timestamp'].to_s;
        event['system_time'] = system_time[0,10];
        event_time = event['timestamp'].to_s;
        event['event_time'] = event_time[0,10];"
    }
    if [event_time] not in [system_time] {
      drop {}
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "system_time","event_time","message","tags","offset" ]
    }
  }
  # adsquare
  else if [type] == "adsquarelog" {
    grok {
      patterns_dir => "/etc/logstash/conf.d/patterns"
      match => [ "message", "%{LOG_HOUR_ADSQUARE:hour}%{SPACE}\[%{DATA:thread}\]%{SPACE}%{WORD:severity}%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:info}"]
    }
    # On supprime les champs qui ne nous interesse pas
    mutate {
      remove_field => [ "message","tags","offset" ]
    }
  }
  # TEST
  else if [type] == "fakelog" {

#    multiline {
#      #pattern => "(^])|(^org.+)|(^backtype.+)|(^com.+)|(^java.+)|(^\s)|(^.+Exception: .+)|(^\s+at .+)|(^\s+... \d+ more)|(^Causes by:)|(^\s*Caused by:.+)"
#      patterns_dir => "/etc/logstash/conf.d/patterns"
#      pattern => "%{LOG_DATE_COMMON:log_date}"
#      negate => true
#      what => "previous"
#    }
#
#    grok {
#       patterns_dir => "/etc/logstash/conf.d/patterns"
#       match => [ "message", "%{LOG_DATE_COMMON:log_date}"]
#       #match => [ "message", "\[%{LOG_DATE:log_date}\]\[%{LOGLEVEL:log_level} \]\[%{NOTSPACE:classname}\] %{GREEDYDATA:msg}"]
#    }
#
#    date {
#       match => ["log_date","YYYY-MM-dd HH:mm:ss"]
#    }
     
     json {
      source => "message"
     }
#     multiline {
#      patterns_dir => "/etc/logstash/conf.d/patterns"
#      pattern => "%{LOG_DATE_COMMON:timestamp}"
#      negate => true
#      what => "previous"
#     }
#     grok {
#       patterns_dir => "/etc/logstash/conf.d/patterns"
#       match => [ "message", "%{LOG_DATE_COMMON:timestamp}%{SPACE}%{WORD:severity}%{SPACE}%{SPACE}%{WORD:class}:%{NUMBER:line}%{SPACE}-%{SPACE}%{DATA:content}({({[^}]+},?\s*)*})?\s*$(?<stacktrace>(?m:.*))?" ]
#     }
  }

################## ELASTICSEARCH
#    multiline {
#      #pattern => "(^])|(^org.+)|(^backtype.+)|(^com.+)|(^java.+)|(^\s)|(^.+Exception: .+)|(^\s+at .+)|(^\s+... \d+ more)|(^Causes by:)|(^\s*Caused by:.+)"
#      patterns_dir => "/etc/logstash/conf.d/patterns"
#      pattern => "%{LOG_DATE:log_date}"
#      negate => true
#      what => "previous"
#    }
#
##[2014-06-27 08:30:06,504][WARN ][net.ldmobile.tracker.es.ElasticTrackerRestAction] [Pretty Persuasions] Bad Request for /display/1704/original/320x50-TIR.jpg
#
#    grok {
#       patterns_dir => "/etc/logstash/conf.d/patterns"
#       match => [ "message", "%{LOG_DATE:log_date}"]
#       #match => [ "message", "\[%{LOG_DATE:log_date}\]\[%{LOGLEVEL:log_level} \]\[%{NOTSPACE:classname}\] %{GREEDYDATA:msg}"]
#    }
#
#    date {
#       match => ["log_date","YYYY-MM-dd HH:mm:ss,SSS"]
#    }
##################

##################
#    grep {
#      add_field => { "message" => "Netadge BO accessed from %{host} %{@fields} %{type} %{@tags}" }
#      match => { "host" => "dracula7" }
#    }

#    grok {
#	                 #   55.3.244.1   GET            /index.html             15824           0.043
#      match => [ "message", "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" ]
#    }
#{"@source":"unknown","@tags":["request"],"@fields":{"method":"GET","path":"/en/bo/reports/is_up.json","format":"json","controller":"reports","action":"is_up","status":200,"duration":22.24,"view":0.15,"db":1.6,"ip":"78.214.179.86","route":"reports#is_up"},"@timestamp":"2014-08-20T14:17:24.770564+00:00"}
##################

##################
###### ES date filtering/ordering
#    multiline {
#      pattern => "(^])|(^org.+)|(^backtype.+)|(^com.+)|(^java.+)|(^\s)|(^.+Exception: .+)|(^\s+at .+)|(^\s+... \d+ more)|(^Causes by:)|(^\s*Caused by:.+)"
#      what => "previous"
#    }
#
##[2014-06-27 08:30:06,504][WARN ][net.ldmobile.tracker.es.ElasticTrackerRestAction] [Pretty Persuasions] Bad Request for /display/1704/original/320x50-TIR.jpg
#    grok {
#       patterns_dir => "/etc/logstash/conf.d/patterns"
#       match => [ "message", "%{LOG_DATE:log_date}"]
#       #match => [ "message", "\[%{LOG_DATE:log_date}\]\[%{LOGLEVEL:log_level} \]\[%{NOTSPACE:classname}\] %{GREEDYDATA:msg}"]
#    }
#    date {
#       match => ["log_date","YYYY-MM-dd HH:mm:ss,SSS"]
#    }
###### ES date filtering/ordering
##################

##################
#    if "_grokparsefailure" in [tags] {
#      drop { }
#    }

    #date {
    #  match => [ "timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
    #  remove_field => [ "timestamp" ]
    #}

    #multiline {
    #  #pattern => "(^net.ldmobile.+)|(^java.lang.+)|(^\s)|(^.+Exception: .+)|(^\s+at .+)|(^\s+... \d+ more)|(^Causes by:)|(^\s*Caused by:.+)"
    #  pattern => "^20[1-9][0-9]-[0-1][0-9]-[0-3][0-9]\s[0-2][0-9]:[0-5][0-9]:[0-5][0-9],[0-9]+\].*"
    #  what => "next"
    #}
##################
}

